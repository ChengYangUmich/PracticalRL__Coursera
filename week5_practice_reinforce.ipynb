{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChengYangUmich/PracticalRL__Coursera/blob/master/week5_practice_reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjDA2b2LGb5",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in TensorFlow\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqB5NIvALGb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "61bfbb16-00c7-4e6b-9fa3-7d9010e14602"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    %tensorflow_version 1.x\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TowNJWCHLGb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9lOaN9gLGcB",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvmmTe9SLGcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "c9f3dc82-b945-42e7-9830-56df0e8be73a"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa77065ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpma6pGVLGcG",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11SB1DSCLGcH",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkyfBu2ULGcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_rAY9wXLGcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create input variables. We only need <s, a, r> for REINFORCE\n",
        "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
        "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
        "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWghyFcwLGcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "bd9480c0-8e76-4f7a-d227-24995550bd89"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.layers as L\n",
        "import keras\n",
        "\n",
        "network = Sequential()\n",
        "network.add(L.InputLayer(state_dim))\n",
        "network.add(Dense(256,activation='relu'))\n",
        "network.add(Dense(n_actions,activation='linear'))\n",
        "\n",
        "logits = network(ph_states)\n",
        "\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38dUiDgbLGcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize model parameters\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctiyjt1sLGca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    return policy.eval({ph_states: [states]})[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mVbuKU1LGcc",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia4WlFtqLGcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(s)\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions,1,p=action_probs)[0]\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4elpEJ3LGcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHOZTROmLGcj",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X06Ym_9KLGcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    G = rewards[-1]\n",
        "    G_array = [rewards[-1]]\n",
        "    for r in rewards[-2::-1]:\n",
        "      G = r + gamma * G \n",
        "      G_array.append(G)\n",
        "\n",
        "    return G_array[::-1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwp4SIyUTBLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJEgwKzLGcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bb285c2e-5fad-461e-9b98-60ccfb5723ee"
      },
      "source": [
        "print(len(get_cumulative_rewards(range(100))))\n",
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMEsm4R-LGco",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t_bg8pHLGco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyMqF_zKLGcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
        "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
        "# Also recall that we defined ph_cumulative_rewards earlier.\n",
        "\n",
        "J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nojhl7EeLGcw",
        "colab_type": "text"
      },
      "source": [
        "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
        "\n",
        "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgyOiqcxLGcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
        "# being deterministic, harming exploration.\n",
        "\n",
        "entropy = -tf.reduce_mean(policy * log_policy) #<YOUR CODE: compute entropy. Do not forget the sign!>"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSsp8PoiLGcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
        "loss = -(J + 0.1 * entropy)\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxCM6fhDLGc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_session(states, actions, rewards, t_max=1000):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
        "    update.run({\n",
        "        ph_states: states,\n",
        "        ph_actions: actions,\n",
        "        ph_cumulative_rewards: cumulative_rewards,\n",
        "    })\n",
        "    return sum(rewards)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVqOQaymLGc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize optimizer parameters\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVhR8VVDLGc6",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAxGc5uVLGc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "76605103-3bad-434d-ff17-456ee79e9c90"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward: 39.460\n",
            "mean reward: 100.190\n",
            "mean reward: 177.670\n",
            "mean reward: 185.760\n",
            "mean reward: 163.300\n",
            "mean reward: 142.600\n",
            "mean reward: 210.500\n",
            "mean reward: 253.390\n",
            "mean reward: 157.640\n",
            "mean reward: 280.780\n",
            "mean reward: 530.050\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b452RJJLGc9",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzSG3MkxLGc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhirPjlxLGdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "5d5ff22c-6399-46b2-c93c-26a3d37f631a"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.2676.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6INs6aRMLGdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "46028754-73d7-4793-91fe-754371edbca1"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'yangche@umich.edu', 'O3vXUlmXph1XmKqc')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 667.92 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcXCkretLGdG",
        "colab_type": "text"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}